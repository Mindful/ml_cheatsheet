\documentclass[11pt]{article}
\usepackage[margin=0.5in]{geometry}

\newcommand{\mnote}[1]{\noindent{\Large{#1}}}






\begin{document}
\section{Probability \& Statistics}
\mnote{$P(A|B) = \frac{P(A \cap B)}{P(B)}$}\\
Probability of A given B is the joint probability of A and B divided by B.\\

\mnote{$P(A \cap B)= P(A|B) \cdot P(B)$}\\
The Chain Rule, inferrable from above. \\

\subsection{Bayes' Rule}
\mnote{$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$} \\
$P(A|B)$ is the posterior probability, informed by the information given by the event $B$ occuring. $P(A)$ is the prior probability. \\

\noindent We can also say that $A$ is our hypothesis and $B$ is our evidence. If I am looking for the chance that I have cancer, with no test it is $A$ but with a test it becomes $P(A|B)$ where $B$ is the outcome of the test.\\

\noindent In the case of a test where a positive result is $B$, Bayes' rule can also be thought of as \\\mnote{$\frac{TP}{TP+FP} = \frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|!A)P(!A)} = \frac{P(B|A)P(A)}{P(B)}$}


\subsection{P-Value}

\section{Information Theory}
TODO: entropy, cross entropy, perplexity


\section{Vocabulary}
\begin{enumerate}
\item \textbf{Precision} is $\frac{TP}{TP + FP}$, or ``how many things you marked positive were actually positive''. 
\item \textbf{Recall} is $\frac{TP}{TP + FN}$, or ``how many of the actually positive things did you mark positive''.
\item  \textbf{Accuracy} is $\frac{TP + TN}{TP + TN + FP + FN}$, or ``how many of the total predictions did you get right''.
\item \textbf{F1 Score} is $(1 + \beta^2) \cdot \frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}$ where recall is $\beta$ times as important as precision. Often $\beta = 1$ for $2 \cdot \frac{precision \cdot recall}{precision + recall}$\\

Precision, Recall and F1 Score all focus on your true positives. F1 is less useful if you need to know about your true negatives.
\item \textbf{Type 1 Error} is a false positive, \textbf{Type 2 Error} is a false negative.
\item \textbf{Bias} is how dissimilar you are to the distribution of the training data, either for reasons of intentional bias introduction or because of a simple model.
\item \textbf{Variance} is how similiar your model distribution is to the training data, which requires that your model is sufficiently complex and models the training data closely.
\item \textbf{Overfit/Underfit} is building a distribution too similar or different to the training data, respectively. Too much bias = underfit. Too much variance = overfit. 
\end{enumerate}

\section{Preventing Overfitting}
k-fold cross validation, l1 and l2 regularization
\subsection{Neural Methods}
l2 reg common
dropout also common
batch norm??

\section{Decision Trees}

\section{Naive Bayes}
multinomial model, binomial model

\section{Regression}
logistic regression vs binomial regression vs linear regression

\section{Support Vector Machines}

\section{Neural Netorks}
backprop
\subsection{Feed-Forward}
Commonly "multi layer perceptron" for classification

\subsection{Long Short-Term Memory}

\subsection{Transformers}

\section{LDA}

\section{Algorithms}
\subsection{Bach Gradient Descent}
\subsection{Stochastic Gradient Descent}

\subsection{Beam Search}

\subsection{TF-IDF}


\end{document}